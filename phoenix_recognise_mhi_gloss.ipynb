{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naserjawas/signpy-ml/blob/main/phoenix_recognise_mhi_gloss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7SKZauh9syL6",
      "metadata": {
        "id": "7SKZauh9syL6"
      },
      "source": [
        "### Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-iL5nk6Ugwtn",
      "metadata": {
        "id": "-iL5nk6Ugwtn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XP1WgQbtuC_P",
      "metadata": {
        "id": "XP1WgQbtuC_P"
      },
      "source": [
        "Directory in Godzilla:\n",
        "\n",
        "```\n",
        "/storage/eng/esrsts/dataset/RWTHPHOENIXWeather2014/new_gloss_files\n",
        "```\n",
        "```\n",
        "/storage/eng/esrsts/dataset/RWTHPHOENIXWeather2014/classifiers\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tKQiIB9W5Ds2",
      "metadata": {
        "id": "tKQiIB9W5Ds2"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/classifiers\n",
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/new_gloss_files/\n",
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/encoded_label/\n",
        "\n",
        "!cp -v /content/drive/MyDrive/new_gloss_files/* /content/dataset/RWTHPHOENIXWeather2014/new_gloss_files/\n",
        "!sh /content/dataset/RWTHPHOENIXWeather2014/new_gloss_files/extractall.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9884d26-9916-4d90-b022-ff911cadb4f0",
      "metadata": {
        "id": "d9884d26-9916-4d90-b022-ff911cadb4f0"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7bc842-4575-4c10-b020-422cba355633",
      "metadata": {
        "id": "ab7bc842-4575-4c10-b020-422cba355633"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "174913a2-5f92-4130-91d8-30c9214b8391",
      "metadata": {
        "id": "174913a2-5f92-4130-91d8-30c9214b8391"
      },
      "source": [
        "### List Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b91e2c-ed51-4848-8a7f-4ab89e71d01c",
      "metadata": {
        "id": "17b91e2c-ed51-4848-8a7f-4ab89e71d01c"
      },
      "outputs": [],
      "source": [
        "def load_filenames(drive, glosstype, glossp, limitsample):\n",
        "    fwdfiles = []\n",
        "    bwdfiles = []\n",
        "    labels = []\n",
        "    rootdir = drive + f\"/new_gloss{glosstype}_files{glossp}p/\"\n",
        "\n",
        "    words = ['__ON__', 'JETZT', 'WETTER', 'WIE-AUSSEHEN', 'MORGEN', 'MITTWOCH', 'ZWEITE', 'FEBRUAR', 'ZEIGEN-BILDSCHIRM', '__OFF__']\n",
        "\n",
        "    for root, dirs, files in os.walk(rootdir):\n",
        "        for name in dirs:\n",
        "            if name.isnumeric():\n",
        "                # limit the sample for training to below 10\n",
        "                if int(name) > limitsample:\n",
        "                    continue\n",
        "                parentdir = root.split(\"/\")[-1]\n",
        "                if parentdir not in words:\n",
        "                    continue\n",
        "                if len(parentdir) < 2:\n",
        "                    continue\n",
        "                if '-' in parentdir:\n",
        "                    continue\n",
        "                if '_' in parentdir:\n",
        "                    continue\n",
        "                labels.append(parentdir)\n",
        "            else:\n",
        "                continue\n",
        "            dirname = os.path.join(root, name)\n",
        "            # print(dirname)\n",
        "            fwdfile = dirname + f\"/allmcmmhif{glossp}p.png\"\n",
        "            if os.path.exists(fwdfile):\n",
        "                fwdfiles.append(fwdfile)\n",
        "            bwdfile = dirname + f\"/allmcmmhib{glossp}p.png\"\n",
        "            if os.path.exists(bwdfile):\n",
        "                bwdfiles.append(bwdfile)\n",
        "\n",
        "    print(f\"gloss{glosstype}_files{glossp}p: {len(fwdfiles)}, {len(bwdfiles)}, {len(labels)}\")\n",
        "\n",
        "    return fwdfiles, bwdfiles, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a3988f-7e9f-4d83-9b21-75564f926262",
      "metadata": {
        "id": "63a3988f-7e9f-4d83-9b21-75564f926262"
      },
      "outputs": [],
      "source": [
        "def get_point_cloud_new(mhi, step, region):\n",
        "    ih, iw = mhi.shape\n",
        "    if region == 'all':\n",
        "        starty = 0\n",
        "        startx = 0\n",
        "        maxy = ih\n",
        "        maxx = iw\n",
        "    elif region == 'left':\n",
        "        starty = 0\n",
        "        startx = 0\n",
        "        maxy = ih\n",
        "        maxx = iw // 2\n",
        "    elif region == 'right':\n",
        "        starty = 0\n",
        "        startx = iw // 2\n",
        "        maxy = ih\n",
        "        maxx = iw\n",
        "\n",
        "    maxy -= step\n",
        "    maxx -= step\n",
        "    points = []\n",
        "    for y in range(starty, maxy, step):\n",
        "        for x in range(startx, maxx, step):\n",
        "            crop = mhi[y:y+step, x:x+step]\n",
        "            maxval = np.max(crop)\n",
        "            points.append(maxval)\n",
        "\n",
        "    return points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6908e5f5-2e82-449c-b3c3-afd4b0c06843",
      "metadata": {
        "id": "6908e5f5-2e82-449c-b3c3-afd4b0c06843"
      },
      "outputs": [],
      "source": [
        "def load_features(filenames):\n",
        "    points = []\n",
        "    for f in filenames:\n",
        "        gimg = cv.imread(f, cv.IMREAD_GRAYSCALE)\n",
        "        gpts = get_point_cloud_new(gimg, 4, 'all')\n",
        "        points.append(gpts)\n",
        "\n",
        "    return points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51524655-5e3e-4948-8fbc-6ff60341d490",
      "metadata": {
        "id": "51524655-5e3e-4948-8fbc-6ff60341d490"
      },
      "outputs": [],
      "source": [
        "def calculate_weight(classes, samples):\n",
        "    weight = {}\n",
        "    num_samples = len(samples)\n",
        "    num_classes = len(classes)\n",
        "\n",
        "    for c in classes:\n",
        "        num_c = samples.count(c)\n",
        "        if num_c > 0:\n",
        "            weight[c] = num_samples / (num_classes * num_c)\n",
        "        else:\n",
        "            weight[c] = 0\n",
        "\n",
        "    return weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e982e3-a32f-4d5d-9f25-937b4904cfbd",
      "metadata": {
        "id": "f8e982e3-a32f-4d5d-9f25-937b4904cfbd"
      },
      "outputs": [],
      "source": [
        "def save_clf(clfobj, drive, glosstype, glossp, mhitype, limitsample):\n",
        "    # rootdir = drive + f\"/new_gloss{glosstype}_files{glossp}p/\"\n",
        "    rootdir = drive + \"/classifiers/\"\n",
        "    if not os.path.exists(rootdir):\n",
        "        os.mkdir(rootdir)\n",
        "    filename = rootdir + f\"j{limitsample}clf{glosstype}_{glossp}{mhitype}\"\n",
        "    with open(filename, 'wb') as pf:\n",
        "        pickle.dump(clfobj, pf)\n",
        "    print(filename, \"saved...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f70ceb5-a4b5-4c7f-ad94-ee608d95886d",
      "metadata": {
        "id": "1f70ceb5-a4b5-4c7f-ad94-ee608d95886d"
      },
      "outputs": [],
      "source": [
        "def load_clf(drive, glosstype, glossp, mhitype, limitsample):\n",
        "    rootdir = drive + f\"/new_gloss{glosstype}_files{glossp}p/\"\n",
        "    filename = rootdir + f\"j{limitsample}clf{glosstype}_{glossp}{mhitype}\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(\"Classifier:\", filename, \"does not exist\")\n",
        "        exit()\n",
        "    with open(filename, 'rb') as pf:\n",
        "        clfobj = pickle.load(pf)\n",
        "    print(filename, \"loaded...\")\n",
        "    print(clfobj)\n",
        "\n",
        "    return clfobj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24c0814-73ba-4a13-864e-01557609cacb",
      "metadata": {
        "id": "b24c0814-73ba-4a13-864e-01557609cacb"
      },
      "outputs": [],
      "source": [
        "def add_results(results, classname, score, cw, ts):\n",
        "    \"\"\" Add individual result into one variable that contains all results.\n",
        "    \"\"\"\n",
        "    if ts == 0:\n",
        "        classweight = cw[classname]\n",
        "        score = score * classweight\n",
        "    if classname in results:\n",
        "        results[classname] += score\n",
        "    else:\n",
        "        results[classname] = score\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50b0e7fd-ad37-44c3-9ef9-318355adc2ee",
      "metadata": {
        "id": "50b0e7fd-ad37-44c3-9ef9-318355adc2ee"
      },
      "outputs": [],
      "source": [
        "def add_summary(summary, results, startidx, endidx):\n",
        "    \"\"\" Create summary from the results.\n",
        "    \"\"\"\n",
        "    # print(results)\n",
        "    for classname, score in results.items():\n",
        "        if classname in summary:\n",
        "            summary[classname].append((startidx, endidx, score))\n",
        "        else:\n",
        "            summary[classname] = [(startidx, endidx, score)]\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d5df8f-1f94-4043-8d3c-a8ce46e75acd",
      "metadata": {
        "id": "68d5df8f-1f94-4043-8d3c-a8ce46e75acd"
      },
      "outputs": [],
      "source": [
        "def find_max_summary(summary, summarycount, start, stop):\n",
        "    \"\"\" Find maximum value from summary\n",
        "    \"\"\"\n",
        "    numcls = 24\n",
        "    # print(start, stop)\n",
        "    topvalue = []\n",
        "    for key, value in summary.items():\n",
        "        indvalue = []\n",
        "        value = sorted(value, key = lambda x: x[2], reverse=True)\n",
        "        valuecount = summarycount[key]\n",
        "        if len(value) > 0:\n",
        "            v = value[0]\n",
        "            vc = list(filter(lambda x: (x[0]==v[0] and x[1]==v[1]), valuecount))[0]\n",
        "            # print(key, v, vc)\n",
        "            # distance of occurance weight: v[0] to v[1]\n",
        "            if ((stop - start) - (v[1] - v[0])) > 0:\n",
        "                wo =  (stop - start) / ((stop - start) - (v[1] - v[0]))\n",
        "            else:\n",
        "                wo = 0\n",
        "\n",
        "            # distance from start weight: v[0] to v_min\n",
        "            if ((stop - start) - (start - v[0])) > 0:\n",
        "                ws = ((stop - start) / ((stop - start) - (start - v[0])))\n",
        "            else:\n",
        "                ws = 0\n",
        "\n",
        "            # number of classifier weight: vc[2]\n",
        "            if (numcls - vc[2]) > 0:\n",
        "                wc =  numcls / (numcls - vc[2])\n",
        "            else:\n",
        "                wc = numcls\n",
        "\n",
        "            # calculate total value\n",
        "            totalv = v[2]\n",
        "            totalv = totalv * (wo + ws + wc)\n",
        "            # print(key, wo, ws, wc, v[2], vc[2], totalv)\n",
        "            topvalue.append((key, v[0], v[1], totalv))\n",
        "\n",
        "    # sorted it to find the best of the best individual\n",
        "    topvalue = sorted(topvalue, key = lambda x:x[3], reverse=True)\n",
        "    # print(\"topvalue:\")\n",
        "    # for t in topvalue:\n",
        "    #     print(t)\n",
        "\n",
        "    return topvalue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79cd4eed-8a98-4ebf-a523-59ef0abbe53b",
      "metadata": {
        "id": "79cd4eed-8a98-4ebf-a523-59ef0abbe53b"
      },
      "outputs": [],
      "source": [
        "def ngram_proba(ngrams, gloss1=\"\", gloss2=\"\", gloss3=\"\"):\n",
        "    f1 = []\n",
        "    f2 = []\n",
        "    f3 = []\n",
        "    if gloss1 != \"\" and gloss2 == \"\" and gloss3 == \"\":\n",
        "        f1 = list(filter(lambda x: (x[0]==gloss1), ngrams))\n",
        "        # print(gloss1, len(f1), len(ngrams))\n",
        "        if len(ngrams) > 0:\n",
        "            return (gloss1, (len(f1)/len(ngrams)))\n",
        "        else:\n",
        "            return (gloss1, 0.0)\n",
        "    elif gloss1 != \"\" and gloss2 != \"\" and gloss3 == \"\":\n",
        "        f1 = list(filter(lambda x: (x[0]==gloss1), ngrams))\n",
        "        f2 = list(filter(lambda x: (x[1]==gloss2), f1))\n",
        "        # print(gloss2, gloss1, len(f2), len(f1))\n",
        "        if len(f1) > 0:\n",
        "            return (gloss2, (len(f2)/len(f1)))\n",
        "        else:\n",
        "            return (gloss2, 0.0)\n",
        "    else:\n",
        "        f1 = list(filter(lambda x: (x[0]==gloss1), ngrams))\n",
        "        f2 = list(filter(lambda x: (x[1]==gloss2), f1))\n",
        "        f3 = list(filter(lambda x: (x[2]==gloss3), f2))\n",
        "        # print(gloss3, gloss2, gloss1, len(f3), len(f2))\n",
        "        if len(f2) > 0:\n",
        "            return (gloss3, (len(f3)/len(f2)))\n",
        "        else:\n",
        "            return (gloss3, 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea9cbfa-966b-49ae-b6d3-9cff8eba584b",
      "metadata": {
        "id": "9ea9cbfa-966b-49ae-b6d3-9cff8eba584b"
      },
      "outputs": [],
      "source": [
        "def load_allmcm_video(datadir, dataname, glosstype, glossp, blankframe):\n",
        "    dataloc = datadir + dataname + f\"/1/allmcm{glossp}p*-{glosstype}.png\"\n",
        "    datafiles = sorted(glob.glob(dataloc))\n",
        "    dataframesori = [cv.imread(filename, cv.IMREAD_GRAYSCALE)\n",
        "                     for filename in datafiles]\n",
        "    dataframesori.insert(0, blankframe)\n",
        "\n",
        "    dataframes = [cv.resize(frame, (iw, ih))\n",
        "                  for frame in dataframesori ]\n",
        "\n",
        "    return dataframes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb250ca-c852-41ad-8c73-024d84c7e8c0",
      "metadata": {
        "id": "8fb250ca-c852-41ad-8c73-024d84c7e8c0"
      },
      "outputs": [],
      "source": [
        "def create_mhi(imgfiles):\n",
        "    numfiles = len(imgfiles)\n",
        "    mhif = np.zeros_like(imgfiles[0])\n",
        "    for i, img in enumerate(imgfiles):\n",
        "        mhif[img > 0] = (((i + 1) / numfiles) * 255)\n",
        "    mhib = np.zeros_like(imgfiles[0])\n",
        "    mhipointsb = get_point_cloud_new(mhib, 4, 'all')\n",
        "    mhipointsb = np.array(mhipointsb).reshape(1, -1)\n",
        "    for i, img in enumerate(reversed(imgfiles)):\n",
        "        mhib[img > 0] = (((i + 1) / numfiles) * 255)\n",
        "    mhipointsf = get_point_cloud_new(mhif, 4, 'all')\n",
        "    mhipointsf = np.array(mhipointsf).reshape(1, -1)\n",
        "\n",
        "    return mhipointsf, mhipointsb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd517220-aa47-4156-abac-c98c01b2e3c7",
      "metadata": {
        "id": "dd517220-aa47-4156-abac-c98c01b2e3c7"
      },
      "outputs": [],
      "source": [
        "def find_path(dataori, dataname, glossresult, ngrams, peaks, cw, allmcm, cf, cb):\n",
        "    # loop for starting point to end point\n",
        "    imgfiles = []\n",
        "    imgfids = []\n",
        "\n",
        "    f = []\n",
        "\n",
        "    results1 = {}\n",
        "    results2 = {}\n",
        "    proclen = 100\n",
        "    procstep = 15\n",
        "    loopstarted = True\n",
        "    start_i = 0\n",
        "    while loopstarted:\n",
        "        summary3 = {}\n",
        "        summary4 = {}\n",
        "        stop_i = start_i + procstep\n",
        "        if stop_i > len(peaks)-1:\n",
        "            stop_i = len(peaks)-1\n",
        "\n",
        "        for i in range(start_i, stop_i):\n",
        "            # print(\"i:\", i)\n",
        "            startpoint = peaks[i]\n",
        "            start_j = i + 1\n",
        "            # stop_j = start_j + procstep\n",
        "            stop_j = stop_i + 1\n",
        "            if stop_j > len(peaks):\n",
        "                stop_j = len(peaks)\n",
        "\n",
        "            for j in range(start_j, stop_j):\n",
        "                # print(\"i-j:\",i, j)\n",
        "                endpoint = peaks[j]\n",
        "                if (endpoint - startpoint) == 1:\n",
        "                    continue\n",
        "                imgfiles = []\n",
        "                imgfids = []\n",
        "\n",
        "                f = []\n",
        "\n",
        "                results1 = {}\n",
        "                results2 = {}\n",
        "                results3 = {}\n",
        "                results4 = {}\n",
        "                # print(\"startpoint - endpoint:\", startpoint, endpoint)\n",
        "\n",
        "                for fid, frame in enumerate(dataori):\n",
        "                    if fid < startpoint:\n",
        "                        continue\n",
        "                    elif fid >= endpoint:\n",
        "                        break\n",
        "                    # allmcm data processing.\n",
        "                    imgfids.append(fid)\n",
        "                    imgfiles.append(frame)\n",
        "\n",
        "                    f.append(allmcm[fid])\n",
        "\n",
        "                # print(\"imgfids:\", imgfids)\n",
        "\n",
        "                mhif, mhib = create_mhi(f)\n",
        "\n",
        "                predf = cf.predict(mhif)\n",
        "                predfpb = cf.predict_proba(mhif)\n",
        "                predb = cb.predict(mhib)\n",
        "                predbpb = cb.predict_proba(mhib)\n",
        "\n",
        "                results1 = add_results(results1, le.inverse_transform(predf)[0], np.max(predfpb), cw, 0)\n",
        "                results2 = add_results(results2, le.inverse_transform(predf)[0], 1, cw, 1)\n",
        "                results3 = add_results(results3, le.inverse_transform(predf)[0], np.max(predfpb), cw, 0)\n",
        "                results4 = add_results(results4, le.inverse_transform(predf)[0], 1, cw, 1)\n",
        "                results1 = add_results(results1, le.inverse_transform(predb)[0], np.max(predbpb), cw, 0)\n",
        "                results2 = add_results(results2, le.inverse_transform(predb)[0], 1, cw, 1)\n",
        "                results3 = add_results(results3, le.inverse_transform(predb)[0], np.max(predbpb), cw, 0)\n",
        "                results4 = add_results(results4, le.inverse_transform(predb)[0], 1, cw, 1)\n",
        "\n",
        "                summary3 = add_summary(summary3, results3, i, j)\n",
        "                summary4 = add_summary(summary4, results4, i, j)\n",
        "\n",
        "        # print(\"summary3:\", summary3)\n",
        "        # for k,v in summary3.items():\n",
        "        #     print(k, len(v))\n",
        "        # print(\"summary4:\", summary4)\n",
        "        # for k,v in summary4.items():\n",
        "        #     print(k, len(v))\n",
        "\n",
        "        # analyse the data for the loop.\n",
        "        # try to find the highest value with\n",
        "        # the longest improvement\n",
        "        topvalue = find_max_summary(summary3, summary4, start_i, stop_i)\n",
        "\n",
        "        # if len(glossresult) > 1:\n",
        "        if len(glossresult) < 0:\n",
        "            # get top ngram\n",
        "            topngram = []\n",
        "            if len(glossresult) == 0:\n",
        "                # print(\"1-gram\")\n",
        "                for t in topvalue:\n",
        "                    ngram = ngram_proba(ngrams, t[0])\n",
        "                    topngram.append(ngram)\n",
        "            elif len(glossresult) == 1:\n",
        "                # print(\"2-gram\")\n",
        "                for t in topvalue:\n",
        "                    ngram = ngram_proba(ngrams, glossresult[-1][0], t[0])\n",
        "                    topngram.append(ngram)\n",
        "            else:\n",
        "                # print(\"3-gram\")\n",
        "                for t in topvalue:\n",
        "                    ngram = ngram_proba(ngrams, glossresult[-2][0], glossresult[-1][0], t[0])\n",
        "                    topngram.append(ngram)\n",
        "            topngram = sorted(topngram, key=lambda x:x[1], reverse=True)\n",
        "            # print(\"topngram:\")\n",
        "            # for t in topngram:\n",
        "            #     print(t)\n",
        "\n",
        "            # top value x top ngram\n",
        "            newtopvalue = []\n",
        "            for tv in topvalue:\n",
        "                tng = list(filter(lambda x: (x[0]==tv[0]), topngram))\n",
        "                if len(tng) > 0 and tng[0][1] > 0:\n",
        "                    newtv = tv[3]*tng[0][1]\n",
        "                    newtopvalue.append((tv[0], tv[1], tv[2], newtv))\n",
        "\n",
        "            newtopvalue = sorted(newtopvalue, key=lambda x:x[3], reverse=True)\n",
        "            if len(newtopvalue) > 3:\n",
        "                if newtopvalue[-1] != newtopvalue[-2] and newtopvalue[-1] != newtopvalue[-3]:\n",
        "                    topvalue = newtopvalue\n",
        "\n",
        "            # print(\"new topvalue:\")\n",
        "            # for tv in topvalue:\n",
        "            #     print(tv)\n",
        "\n",
        "        if len(topvalue) > 0:\n",
        "            glossresult.append(topvalue[0])\n",
        "            start_i = topvalue[0][2]\n",
        "        else:\n",
        "            start_i = start_i + 1\n",
        "\n",
        "        print()\n",
        "        print(\"Video:\", dataname)\n",
        "        print(\"glossresult:\")\n",
        "        for g in glossresult:\n",
        "            print(g[0])\n",
        "\n",
        "        if start_i >= len(peaks):\n",
        "            loopstarted = False\n",
        "\n",
        "        # c = input()\n",
        "\n",
        "        if c == \"n\":\n",
        "            loopstarted = False\n",
        "\n",
        "    return glossresult"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c8caa24-f25c-4bf5-83be-f407f76b4ba6",
      "metadata": {
        "id": "4c8caa24-f25c-4bf5-83be-f407f76b4ba6"
      },
      "source": [
        "### Mode Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb894fdb-a46c-4fce-95fb-e11b1cc03e5d",
      "metadata": {
        "id": "bb894fdb-a46c-4fce-95fb-e11b1cc03e5d"
      },
      "outputs": [],
      "source": [
        "train = True\n",
        "limitsample = 100\n",
        "glosstype = 2\n",
        "glossp = 100\n",
        "drive = \"/content/dataset/RWTHPHOENIXWeather2014\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b2d196e-90b6-4f16-94e8-7ef2ef5901a5",
      "metadata": {
        "id": "5b2d196e-90b6-4f16-94e8-7ef2ef5901a5"
      },
      "source": [
        "### Load Filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "316069a1-898a-4ead-ae72-962052d36e57",
      "metadata": {
        "id": "316069a1-898a-4ead-ae72-962052d36e57"
      },
      "outputs": [],
      "source": [
        "gf, gb, gl = load_filenames(drive, glosstype, glossp, limitsample)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae3bc70b-c77e-4586-8fdb-f751f9176223",
      "metadata": {
        "id": "ae3bc70b-c77e-4586-8fdb-f751f9176223"
      },
      "source": [
        "### Label Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "852d92e8-9462-4dda-bfb8-9791d78b780a",
      "metadata": {
        "id": "852d92e8-9462-4dda-bfb8-9791d78b780a"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "le.fit(gl)\n",
        "# for c in le.classes_:\n",
        "#     print(c)\n",
        "print(len(le.classes_))\n",
        "print(le.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lRoQO9reFKxS",
      "metadata": {
        "id": "lRoQO9reFKxS"
      },
      "outputs": [],
      "source": [
        "with open(f\"/content/dataset/RWTHPHOENIXWeather2014/encoded_label/label_le_obj{limitsample}\", \"wb\") as pf:\n",
        "    pickle.dump(le, pf)\n",
        "pf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0fbf93f-c6b0-440a-898b-356852ddb434",
      "metadata": {
        "id": "f0fbf93f-c6b0-440a-898b-356852ddb434"
      },
      "source": [
        "### Load Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b827a2a-3845-4738-a0cc-0a642e74f319",
      "metadata": {
        "id": "9b827a2a-3845-4738-a0cc-0a642e74f319"
      },
      "outputs": [],
      "source": [
        "ff = np.array(load_features(gf))\n",
        "fb = np.array(load_features(gb))\n",
        "lbl = le.transform(gl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d7c2b55-6dec-4dce-ab74-ee14aedae2e0",
      "metadata": {
        "id": "5d7c2b55-6dec-4dce-ab74-ee14aedae2e0"
      },
      "source": [
        "### Remove Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a76aee9e-f1b3-408d-9e0e-866100c299a1",
      "metadata": {
        "id": "a76aee9e-f1b3-408d-9e0e-866100c299a1"
      },
      "outputs": [],
      "source": [
        "print(ff.shape, lbl.shape)\n",
        "\n",
        "od = OneClassSVM()\n",
        "# od = IsolationForest(n_estimators=1000)\n",
        "nzf = [np.count_nonzero(f) for f in ff]\n",
        "nzf = np.array(nzf)\n",
        "nzf = nzf.reshape(-1, 1)\n",
        "mask = od.fit_predict(nzf)\n",
        "ff = ff[mask == 1]\n",
        "fb = fb[mask == 1]\n",
        "lbl = lbl[mask == 1]\n",
        "print(\"mask1:\", ff.shape, lbl.shape)\n",
        "listfiles = np.array(gf)\n",
        "listfiles = listfiles[mask == 1]\n",
        "\n",
        "mask = od.fit_predict(ff)\n",
        "ff = ff[mask == 1]\n",
        "fb = fb[mask == 1]\n",
        "lbl = lbl[mask == 1]\n",
        "print(\"mask2:\", ff.shape, lbl.shape)\n",
        "listfiles = listfiles[mask == 1]\n",
        "print(listfiles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lnQNw8X4tUZm",
      "metadata": {
        "id": "lnQNw8X4tUZm"
      },
      "outputs": [],
      "source": [
        "with open(f\"/content/dataset/RWTHPHOENIXWeather2014/encoded_label/label_sample{limitsample}\", \"wb\") as pf:\n",
        "    pickle.dump(lbl,pf)\n",
        "pf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c4ca870-ccd6-4e05-a85c-91b1e68780ae",
      "metadata": {
        "id": "9c4ca870-ccd6-4e05-a85c-91b1e68780ae"
      },
      "source": [
        "### Calculate Weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e550e1d3-c975-445b-965e-18ab5c586578",
      "metadata": {
        "id": "e550e1d3-c975-445b-965e-18ab5c586578"
      },
      "outputs": [],
      "source": [
        "cw = calculate_weight(list(le.classes_), list(le.inverse_transform(lbl)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D3-oVC5evvNu",
      "metadata": {
        "id": "D3-oVC5evvNu"
      },
      "outputs": [],
      "source": [
        "with open(f\"/content/dataset/RWTHPHOENIXWeather2014/encoded_label/label_weight{limitsample}\", \"wb\") as pf:\n",
        "    pickle.dump(cw,pf)\n",
        "pf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70902337-f6f0-43a6-a3b0-3287da2b1b88",
      "metadata": {
        "id": "70902337-f6f0-43a6-a3b0-3287da2b1b88"
      },
      "source": [
        "### Train Classifier or Load Pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db641890-0ffa-4b2c-b656-94f1ec9389e5",
      "metadata": {
        "id": "db641890-0ffa-4b2c-b656-94f1ec9389e5"
      },
      "outputs": [],
      "source": [
        "if train:\n",
        "    cf = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
        "    cb = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
        "    cf.fit(ff, lbl)\n",
        "    cb.fit(fb, lbl)\n",
        "    save_clf(cf, drive, glosstype, glossp, 'f', limitsample)\n",
        "    save_clf(cb, drive, glosstype, glossp, 'b', limitsample)\n",
        "else:\n",
        "    cf  = load_clf(drive, glosstype, glossp, 'f', limitsample)\n",
        "    cb  = load_clf(drive, glosstype, glossp, 'b', limitsample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PEyeThDG3-WD",
      "metadata": {
        "id": "PEyeThDG3-WD"
      },
      "outputs": [],
      "source": [
        "!cp -v /content/dataset/RWTHPHOENIXWeather2014/classifiers/* /content/drive/MyDrive/classifiers/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -v /content/dataset/RWTHPHOENIXWeather2014/encoded_label/* /content/drive/MyDrive/encoded_label/"
      ],
      "metadata": {
        "id": "Z7CpjIYHc_83"
      },
      "id": "Z7CpjIYHc_83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "188a3f37-e932-47c1-bd46-0b9a7c105525",
      "metadata": {
        "id": "188a3f37-e932-47c1-bd46-0b9a7c105525"
      },
      "source": [
        "### Load Test Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e24387d-45cf-4298-bfa3-8d48206e42d2",
      "metadata": {
        "id": "8e24387d-45cf-4298-bfa3-8d48206e42d2"
      },
      "outputs": [],
      "source": [
        "drive = \"/storage/eng/esrsts/dataset/RWTHPHOENIXWeather2014\"\n",
        "datadir = drive + \"/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/test/\"\n",
        "datanames = sorted(os.listdir(datadir))\n",
        "if datanames[0] == \".DS_Store\":\n",
        "    datanames.pop(0)\n",
        "print(len(datanames), \"data available...\")\n",
        "iw, ih = 210, 300\n",
        "blankframe = np.zeros((iw, ih), dtype=np.uint8)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6dc739-3a3d-4dfe-ac70-044b5a496f7a",
      "metadata": {
        "id": "fb6dc739-3a3d-4dfe-ac70-044b5a496f7a"
      },
      "source": [
        "### Load n-gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0bc2ae0-32ea-4bca-a383-ee8346b174a1",
      "metadata": {
        "id": "a0bc2ae0-32ea-4bca-a383-ee8346b174a1"
      },
      "outputs": [],
      "source": [
        "with open(\"/storage/eng/esrsts/phoenix_3grams_all\", \"rb\") as pf:\n",
        "    ngrams = pickle.load(pf)\n",
        "pf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a17e2f83-97f5-48f3-9659-b367b36a2452",
      "metadata": {
        "id": "a17e2f83-97f5-48f3-9659-b367b36a2452"
      },
      "source": [
        "### Load Segment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cceb17f9-8e05-49e4-b165-71f4fe1dbdaf",
      "metadata": {
        "id": "cceb17f9-8e05-49e4-b165-71f4fe1dbdaf"
      },
      "outputs": [],
      "source": [
        "with open(f\"/storage/eng/esrsts/alldata_test_{glossp}p_{glosstype}\", \"rb\") as pf:\n",
        "    alldata = pickle.load(pf)\n",
        "pf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6af618c-4b0d-49f5-b6a0-eb2fe07fe9f3",
      "metadata": {
        "id": "a6af618c-4b0d-49f5-b6a0-eb2fe07fe9f3"
      },
      "source": [
        "### Recognition Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00d85d2a-7e6c-4423-911d-7d5f2950a531",
      "metadata": {
        "id": "00d85d2a-7e6c-4423-911d-7d5f2950a531"
      },
      "outputs": [],
      "source": [
        "datanamespart = datanames[1:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e88c5ac-6929-41a8-b8bb-77e9e362454a",
      "metadata": {
        "id": "7e88c5ac-6929-41a8-b8bb-77e9e362454a"
      },
      "outputs": [],
      "source": [
        "finalresults = []\n",
        "for dataname_i, dataname in enumerate(datanamespart):\n",
        "    # load original data frames\n",
        "    dataloc = datadir + dataname + f\"/1/*-0.png\"\n",
        "    datafiles = sorted(glob.glob(dataloc))\n",
        "    lendatafiles = len(datafiles)\n",
        "    print(\"(\", dataname_i + 1, \"/\", len(datanamespart),\")\",\n",
        "          dataname, \"has\", lendatafiles, \"frames\")\n",
        "    imgori = [cv.imread(filename, cv.IMREAD_COLOR)\n",
        "              for filename in datafiles]\n",
        "    dataori = [cv.resize(img, (iw, ih))\n",
        "               for img in imgori]\n",
        "    peaks = alldata[dataname]\n",
        "    peaks.insert(0, 0)\n",
        "    peaks.append(lendatafiles-1)\n",
        "    allmcm = load_allmcm_video(datadir, dataname, glosstype, glossp, blankframe)\n",
        "    glossresult1 = []\n",
        "\n",
        "    glossresult1 = find_path(dataori, dataname, glossresult1, ngrams, peaks, cw, allmcm, cf, cb)\n",
        "    listgloss = [g[0] for g in glossresult1]\n",
        "    finalresults.append((dataname, listgloss))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2105f1-721a-4dc5-a494-07fd01fa353b",
      "metadata": {
        "id": "aa2105f1-721a-4dc5-a494-07fd01fa353b"
      },
      "source": [
        "### Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73eb3e0-a895-4208-9b25-6885f43e616e",
      "metadata": {
        "id": "c73eb3e0-a895-4208-9b25-6885f43e616e"
      },
      "outputs": [],
      "source": [
        "with open('finalresults', 'wb') as pf:\n",
        "    pickle.dump(finalresults, pf)\n",
        "pf.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}