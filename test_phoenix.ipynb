{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naserjawas/signpy-ml/blob/main/test_phoenix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loPzn98gX6J7"
      },
      "source": [
        "# Test ASLR System using Phoenix Dataset\n",
        "\n",
        "Author: Naser Jawas\n",
        "\n",
        "Created on: 1 August 2024\n",
        "\n",
        "\n",
        "This file is used to test the ASLR system on RWTH-Phoenix Weather dataset. It uses a pre-trained classifiers on our proposed feature. The feature and the training process will be described on separate files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEVQKTLNEoHy"
      },
      "source": [
        "### Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pllF_ivlEL8A"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEnoID7rE829"
      },
      "source": [
        "### Import Files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-SULtr__XS_"
      },
      "source": [
        "**Copy pre-trained files from google drive**\n",
        "\n",
        "The training process is located in *phoenix_recognise_mhi_gloss.py*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrH9_BSoFECg"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/classifiers/\n",
        "#!cp -v /content/drive/MyDrive/classifiers/j10clf1_30[b,f] /content/dataset/RWTHPHOENIXWeather2014/classifiers/\n",
        "#!cp -v /content/drive/MyDrive/classifiers/j10clf1_40[b,f] /content/dataset/RWTHPHOENIXWeather2014/classifiers/\n",
        "!cp -v /content/drive/MyDrive/classifiers/j100clf1_50[b,f] /content/dataset/RWTHPHOENIXWeather2014/classifiers/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVRCmZQ5_dyT"
      },
      "source": [
        "**Copy ngram files**\n",
        "\n",
        "The ngram files are generated using *phoenix_ngram.py*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9nzTPot_g60"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/ngram_files/\n",
        "!cp -v /content/drive/MyDrive/ngram_files/* /content/dataset/RWTHPHOENIXWeather2014/ngram_files/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xzNhJJuCB4c"
      },
      "source": [
        "**Copy segment files**\n",
        "\n",
        "The segment files are generated using *phoenix_segment.py*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdsGPTerCLKe"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/segment_files/\n",
        "!cp -v /content/drive/MyDrive/segment_files/* /content/dataset/RWTHPHOENIXWeather2014/segment_files/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLX_0I6LGzg3"
      },
      "source": [
        "**Copy encoded labels files**\n",
        "\n",
        "The endcoded label object. The original encoding processes are located in *phoenix_recognise_mhi_gloss.py*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSAMwLdHHHCj"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/encoded_label/\n",
        "!cp -v /content/drive/MyDrive/encoded_label/* /content/dataset/RWTHPHOENIXWeather2014/encoded_label/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNMpxMQ3AFZS"
      },
      "source": [
        "**Copy test data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USGlisN87TWn"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADdXv-K7AL6l"
      },
      "outputs": [],
      "source": [
        "!tar -xzf /content/drive/MyDrive/test_data/ready/phoenix_test.tar.gz -C /content/dataset/RWTHPHOENIXWeather2014/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMv48WZV7WzY"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/test_data/data /content/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKoQN40fImWv"
      },
      "source": [
        "**Copy annotation data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnjQThViIlg6"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/dataset/RWTHPHOENIXWeather2014/phoenix2014-release/phoenix-2014-multisigner/annotations/manual/\n",
        "!cp /content/drive/MyDrive/annotation/test /content/dataset/RWTHPHOENIXWeather2014/phoenix2014-release/phoenix-2014-multisigner/annotations/manual/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FGelzymJWCI"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWR4Q5U7JU7X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import glob\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.ensemble import IsolationForest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vdp3tszJaf9"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4-lb1boJcu-"
      },
      "outputs": [],
      "source": [
        "def load_clf(drive, glosstype, glossp, mhitype, limitsample):\n",
        "    rootdir = drive + f\"/classifiers/\"\n",
        "    filename = rootdir + f\"j{limitsample}clf{glosstype}_{glossp}{mhitype}\"\n",
        "    if not os.path.exists(filename):\n",
        "        print(\"Classifier:\", filename, \"does not exist\")\n",
        "        exit()\n",
        "    print(filename)\n",
        "    with open(filename, 'rb') as pf:\n",
        "        clfobj = pickle.load(pf)\n",
        "    print(filename, \"loaded...\")\n",
        "    print(clfobj)\n",
        "\n",
        "    return clfobj\n",
        "\n",
        "\n",
        "def load_allmcm_video(datadir, dataname, glosstype, glossp, blankframe):\n",
        "    dataloc = datadir + dataname + f\"/1/allmcm{glossp}p*-{glosstype}.png\"\n",
        "    datafiles = sorted(glob.glob(dataloc))\n",
        "    dataframesori = [cv.imread(filename, cv.IMREAD_GRAYSCALE)\n",
        "                     for filename in datafiles]\n",
        "    dataframesori.insert(0, blankframe)\n",
        "    dataframes = [cv.resize(frame, (iw, ih))\n",
        "                  for frame in dataframesori]\n",
        "\n",
        "    return dataframes\n",
        "\n",
        "\n",
        "def load_filenames(drive, glosstype, glossp, limitsample):\n",
        "    fwdfiles = []\n",
        "    bwdfiles = []\n",
        "    labels = []\n",
        "    rootdir = drive + f\"/new_gloss{glosstype}_files{glossp}p/\"\n",
        "\n",
        "    for root, dirs, files in os.walk(rootdir):\n",
        "        for name in dirs:\n",
        "            if name.isnumeric():\n",
        "                # limit the sample for training to below 10\n",
        "                if int(name) > limitsample:\n",
        "                    continue\n",
        "                parentdir = root.split(\"/\")[-1]\n",
        "                if len(parentdir) < 2:\n",
        "                    continue\n",
        "                if '-' in parentdir:\n",
        "                    continue\n",
        "                if '_' in parentdir:\n",
        "                    continue\n",
        "                labels.append(parentdir)\n",
        "            else:\n",
        "                continue\n",
        "            dirname = os.path.join(root, name)\n",
        "            # print(dirname)\n",
        "            fwdfile = dirname + f\"/allmcmmhif{glossp}p.png\"\n",
        "            if os.path.exists(fwdfile):\n",
        "                fwdfiles.append(fwdfile)\n",
        "            bwdfile = dirname + f\"/allmcmmhib{glossp}p.png\"\n",
        "            if os.path.exists(bwdfile):\n",
        "                bwdfiles.append(bwdfile)\n",
        "\n",
        "    print(f\"gloss{glosstype}_files{glossp}p: {len(fwdfiles)}, {len(bwdfiles)}, {len(labels)}\")\n",
        "\n",
        "    return fwdfiles, bwdfiles, labels\n",
        "\n",
        "\n",
        "def get_point_cloud_new(mhi, step, region):\n",
        "    ih, iw = mhi.shape\n",
        "    if region == 'all':\n",
        "        starty = 0\n",
        "        startx = 0\n",
        "        maxy = ih\n",
        "        maxx = iw\n",
        "    elif region == 'left':\n",
        "        starty = 0\n",
        "        startx = 0\n",
        "        maxy = ih\n",
        "        maxx = iw // 2\n",
        "    elif region == 'right':\n",
        "        starty = 0\n",
        "        startx = iw // 2\n",
        "        maxy = ih\n",
        "        maxx = iw\n",
        "\n",
        "    maxy -= step\n",
        "    maxx -= step\n",
        "    points = []\n",
        "    for y in range(starty, maxy, step):\n",
        "        for x in range(startx, maxx, step):\n",
        "            crop = mhi[y:y+step, x:x+step]\n",
        "            maxval = np.max(crop)\n",
        "            points.append(maxval)\n",
        "\n",
        "    return points\n",
        "\n",
        "\n",
        "def create_mhi(imgfiles):\n",
        "    numfiles = len(imgfiles)\n",
        "    mhif = np.zeros_like(imgfiles[0])\n",
        "    for i, img in enumerate(imgfiles):\n",
        "        mhif[img > 0] = (((i + 1) / numfiles) * 255)\n",
        "    mhib = np.zeros_like(imgfiles[0])\n",
        "    mhipointsb = get_point_cloud_new(mhib, 4, 'all')\n",
        "    mhipointsb = np.array(mhipointsb).reshape(1, -1)\n",
        "    for i, img in enumerate(reversed(imgfiles)):\n",
        "        mhib[img > 0] = (((i + 1) / numfiles) * 255)\n",
        "    mhipointsf = get_point_cloud_new(mhif, 4, 'all')\n",
        "    mhipointsf = np.array(mhipointsf).reshape(1, -1)\n",
        "\n",
        "    return mhipointsf, mhipointsb\n",
        "\n",
        "\n",
        "def add_results(results, classname, score, cw, ts):\n",
        "    \"\"\" Add individual result into one variable that contains all results.\n",
        "    \"\"\"\n",
        "    if ts == 0:\n",
        "        classweight = cw[classname]\n",
        "        score = score * classweight\n",
        "    if classname in results:\n",
        "        results[classname] += score\n",
        "    else:\n",
        "        results[classname] = score\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def add_summary(summary, results, startidx, endidx):\n",
        "    \"\"\" Create summary from the results.\n",
        "    \"\"\"\n",
        "    # print(results)\n",
        "    for classname, score in results.items():\n",
        "        if classname in summary:\n",
        "            summary[classname].append((startidx, endidx, score))\n",
        "        else:\n",
        "            summary[classname] = [(startidx, endidx, score)]\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "def find_max_summary(summary, summarycount, start, stop):\n",
        "    \"\"\" Find maximum value from summary\n",
        "    \"\"\"\n",
        "    numcls = 24\n",
        "    # print(start, stop)\n",
        "    topvalue = []\n",
        "    for key, value in summary.items():\n",
        "        indvalue = []\n",
        "        value = sorted(value, key = lambda x: x[2], reverse=True)\n",
        "        valuecount = summarycount[key]\n",
        "        if len(value) > 0:\n",
        "            v = value[0]\n",
        "            vc = list(filter(lambda x: (x[0]==v[0] and x[1]==v[1]), valuecount))[0]\n",
        "            # print(key, v, vc)\n",
        "            # distance of occurance weight: v[0] to v[1]\n",
        "            if ((stop - start) - (v[1] - v[0])) > 0:\n",
        "                wo =  (stop - start) / ((stop - start) - (v[1] - v[0]))\n",
        "            else:\n",
        "                wo = 0\n",
        "\n",
        "            # distance from start weight: v[0] to v_min\n",
        "            if ((stop - start) - (start - v[0])) > 0:\n",
        "                ws = ((stop - start) / ((stop - start) - (start - v[0])))\n",
        "            else:\n",
        "                ws = 0\n",
        "\n",
        "            # number of classifier weight: vc[2]\n",
        "            if (numcls - vc[2]) > 0:\n",
        "                wc =  numcls / (numcls - vc[2])\n",
        "            else:\n",
        "                wc = numcls\n",
        "\n",
        "            # calculate total value\n",
        "            totalv = v[2]\n",
        "            totalv = totalv * (wo + ws + wc)\n",
        "            # print(key, wo, ws, wc, v[2], vc[2], totalv)\n",
        "            topvalue.append((key, v[0], v[1], totalv))\n",
        "\n",
        "    # sorted it to find the best of the best individual\n",
        "    topvalue = sorted(topvalue, key = lambda x:x[3], reverse=True)\n",
        "    # print(\"topvalue:\")\n",
        "    # for t in topvalue:\n",
        "    #     print(t)\n",
        "\n",
        "    return topvalue\n",
        "\n",
        "\n",
        "def ngram_proba(ngrams, gloss1=\"\", gloss2=\"\", gloss3=\"\"):\n",
        "    f1 = []\n",
        "    f2 = []\n",
        "    f3 = []\n",
        "    if gloss1 != \"\" and gloss2 == \"\" and gloss3 == \"\":\n",
        "        f1 = list(filter(lambda x: (x[0]==gloss1), ngrams))\n",
        "        # print(gloss1, len(f1), len(ngrams))\n",
        "        if len(ngrams) > 0:\n",
        "            return (gloss1, (len(f1)/len(ngrams)))\n",
        "        else:\n",
        "            return (gloss1, 0.0)\n",
        "    elif gloss1 != \"\" and gloss2 != \"\" and gloss3 == \"\":\n",
        "        f1 = list(filter(lambda x: (x[0]==gloss1), ngrams))\n",
        "        f2 = list(filter(lambda x: (x[1]==gloss2), f1))\n",
        "        # print(gloss2, gloss1, len(f2), len(f1))\n",
        "        if len(f1) > 0:\n",
        "            return (gloss2, (len(f2)/len(f1)))\n",
        "        else:\n",
        "            return (gloss2, 0.0)\n",
        "    else:\n",
        "        f1 = list(filter(lambda x: (x[0]==gloss1), ngrams))\n",
        "        f2 = list(filter(lambda x: (x[1]==gloss2), f1))\n",
        "        f3 = list(filter(lambda x: (x[2]==gloss3), f2))\n",
        "        # print(gloss3, gloss2, gloss1, len(f3), len(f2))\n",
        "        if len(f2) > 0:\n",
        "            return (gloss3, (len(f3)/len(f2)))\n",
        "        else:\n",
        "            return (gloss3, 0.0)\n",
        "\n",
        "\n",
        "def find_path(dataori, dataname, glossresult, ngrams, peaks, cw, allmcm, cf, cb):\n",
        "    # loop for starting point to end point\n",
        "    imgfiles = []\n",
        "    imgfids = []\n",
        "\n",
        "    f = []\n",
        "\n",
        "    results1 = {}\n",
        "    results2 = {}\n",
        "    proclen = 100\n",
        "    procstep = 25\n",
        "    loopstarted = True\n",
        "    start_i = 0\n",
        "    while loopstarted:\n",
        "        summary3 = {}\n",
        "        summary4 = {}\n",
        "        stop_i = start_i + procstep\n",
        "        if stop_i > len(peaks)-1:\n",
        "            stop_i = len(peaks)-1\n",
        "\n",
        "        for i in range(start_i, stop_i):\n",
        "            # print(\"i:\", i)\n",
        "            startpoint = peaks[i]\n",
        "            start_j = i + 1\n",
        "            # stop_j = start_j + procstep\n",
        "            stop_j = stop_i + 1\n",
        "            if stop_j > len(peaks):\n",
        "                stop_j = len(peaks)\n",
        "\n",
        "            for j in range(start_j, stop_j):\n",
        "                # print(\"i-j:\",i, j)\n",
        "                endpoint = peaks[j]\n",
        "                if (endpoint - startpoint) == 1:\n",
        "                    continue\n",
        "                imgfiles = []\n",
        "                imgfids = []\n",
        "\n",
        "                f = []\n",
        "\n",
        "                results1 = {}\n",
        "                results2 = {}\n",
        "                results3 = {}\n",
        "                results4 = {}\n",
        "                # print(\"startpoint - endpoint:\", startpoint, endpoint)\n",
        "\n",
        "                for fid, frame in enumerate(dataori):\n",
        "                    if fid < startpoint:\n",
        "                        continue\n",
        "                    elif fid >= endpoint:\n",
        "                        break\n",
        "                    # allmcm data processing.\n",
        "                    imgfids.append(fid)\n",
        "                    imgfiles.append(frame)\n",
        "\n",
        "                    f.append(allmcm[fid])\n",
        "\n",
        "                # print(\"imgfids:\", imgfids)\n",
        "\n",
        "                mhif, mhib = create_mhi(f)\n",
        "\n",
        "                predf = cf.predict(mhif)\n",
        "                predfpb = cf.predict_proba(mhif)\n",
        "                predb = cb.predict(mhib)\n",
        "                predbpb = cb.predict_proba(mhib)\n",
        "\n",
        "                results1 = add_results(results1, le.inverse_transform(predf)[0], np.max(predfpb), cw, 0)\n",
        "                results2 = add_results(results2, le.inverse_transform(predf)[0], 1, cw, 1)\n",
        "                results3 = add_results(results3, le.inverse_transform(predf)[0], np.max(predfpb), cw, 0)\n",
        "                results4 = add_results(results4, le.inverse_transform(predf)[0], 1, cw, 1)\n",
        "                results1 = add_results(results1, le.inverse_transform(predb)[0], np.max(predbpb), cw, 0)\n",
        "                results2 = add_results(results2, le.inverse_transform(predb)[0], 1, cw, 1)\n",
        "                results3 = add_results(results3, le.inverse_transform(predb)[0], np.max(predbpb), cw, 0)\n",
        "                results4 = add_results(results4, le.inverse_transform(predb)[0], 1, cw, 1)\n",
        "\n",
        "                summary3 = add_summary(summary3, results3, i, j)\n",
        "                summary4 = add_summary(summary4, results4, i, j)\n",
        "\n",
        "        # print(\"summary3:\", summary3)\n",
        "        # for k,v in summary3.items():\n",
        "        #     print(k, len(v))\n",
        "        # print(\"summary4:\", summary4)\n",
        "        # for k,v in summary4.items():\n",
        "        #     print(k, len(v))\n",
        "\n",
        "        # analyse the data for the loop.\n",
        "        # try to find the highest value with\n",
        "        # the longest improvement\n",
        "        topvalue = find_max_summary(summary3, summary4, start_i, stop_i)\n",
        "\n",
        "        if len(glossresult) > 0:\n",
        "        # if len(glossresult) < 0:\n",
        "            # get top ngram\n",
        "            topngram = []\n",
        "            if len(glossresult) == 0:\n",
        "                # print(\"1-gram\")\n",
        "                for t in topvalue:\n",
        "                    ngram = ngram_proba(ngrams, t[0])\n",
        "                    topngram.append(ngram)\n",
        "            elif len(glossresult) == 1:\n",
        "                # print(\"2-gram\")\n",
        "                for t in topvalue:\n",
        "                    ngram = ngram_proba(ngrams, glossresult[-1][0], t[0])\n",
        "                    topngram.append(ngram)\n",
        "            else:\n",
        "                # print(\"3-gram\")\n",
        "                for t in topvalue:\n",
        "                    ngram = ngram_proba(ngrams, glossresult[-2][0], glossresult[-1][0], t[0])\n",
        "                    topngram.append(ngram)\n",
        "            topngram = sorted(topngram, key=lambda x:x[1], reverse=True)\n",
        "            # print(\"topngram:\")\n",
        "            # for t in topngram:\n",
        "            #     print(t)\n",
        "\n",
        "            # top value x top ngram\n",
        "            newtopvalue = []\n",
        "            for tv in topvalue:\n",
        "                tng = list(filter(lambda x: (x[0]==tv[0]), topngram))\n",
        "                if len(tng) > 0 and tng[0][1] > 0:\n",
        "                    newtv = tv[3]*tng[0][1]\n",
        "                    newtopvalue.append((tv[0], tv[1], tv[2], newtv))\n",
        "\n",
        "            newtopvalue = sorted(newtopvalue, key=lambda x:x[3], reverse=True)\n",
        "            if len(newtopvalue) > 3:\n",
        "                if newtopvalue[-1] != newtopvalue[-2] and newtopvalue[-1] != newtopvalue[-3]:\n",
        "                    topvalue = newtopvalue\n",
        "\n",
        "            # print(\"new topvalue:\")\n",
        "            # for tv in topvalue:\n",
        "            #     print(tv)\n",
        "\n",
        "        if len(topvalue) > 0:\n",
        "            glossresult.append(topvalue[0])\n",
        "            start_i = topvalue[0][2]\n",
        "        else:\n",
        "            start_i = start_i + 1\n",
        "\n",
        "        # print()\n",
        "        # print(\"Video:\", dataname)\n",
        "        # print(\"glossresult:\")\n",
        "        # for g in glossresult:\n",
        "        #     print(g[0])\n",
        "\n",
        "        if start_i >= len(peaks):\n",
        "            loopstarted = False\n",
        "\n",
        "        # c = input()\n",
        "\n",
        "        # if c == \"n\":\n",
        "        #     loopstarted = False\n",
        "\n",
        "    return glossresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8xqEh0vJ3ZW"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dodnEaMeJ6EM"
      },
      "outputs": [],
      "source": [
        "limitsample = 100\n",
        "glosstype = 1\n",
        "glossp = 50\n",
        "drive = \"/content/dataset/RWTHPHOENIXWeather2014\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nb3o6gnJhyX"
      },
      "source": [
        "### Main\n",
        "\n",
        "**Load the pre-trained classifiers:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpJwkLfZJLSX"
      },
      "outputs": [],
      "source": [
        "cf = load_clf(drive, glosstype, glossp, 'f', limitsample)\n",
        "cb = load_clf(drive, glosstype, glossp, 'b', limitsample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niRK4K_wkglH"
      },
      "source": [
        "**Load pre-processed objects:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4AaFHzOklBA"
      },
      "outputs": [],
      "source": [
        "# n-gram\n",
        "n = 3\n",
        "with open(f\"/content/dataset/RWTHPHOENIXWeather2014/ngram_files/phoenix_{n}grams\", \"rb\") as pf:\n",
        "    ngrams = pickle.load(pf)\n",
        "pf.close()\n",
        "\n",
        "# label encoder objects\n",
        "with open(f\"/content/dataset/RWTHPHOENIXWeather2014/encoded_label/label_le_obj{limitsample}\", \"rb\") as pf:\n",
        "    le = pickle.load(pf)\n",
        "pf.close()\n",
        "\n",
        "with open(f\"/content/dataset/RWTHPHOENIXWeather2014/encoded_label/label_sample{limitsample}\", \"rb\") as pf:\n",
        "    lbl = pickle.load(pf)\n",
        "pf.close()\n",
        "\n",
        "with open(f\"/content/dataset/RWTHPHOENIXWeather2014/encoded_label/label_weight{limitsample}\", \"rb\") as pf:\n",
        "    cw = pickle.load(pf)\n",
        "pf.close()\n",
        "\n",
        "# segment objects\n",
        "sp = 50\n",
        "st = 1\n",
        "with open(f\"/content/dataset/RWTHPHOENIXWeather2014/segment_files/alldata_test_{sp}p_{st}\", \"rb\") as pf:\n",
        "    segments = pickle.load(pf)\n",
        "pf.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifUBwHQrgmY9"
      },
      "source": [
        "**Load test data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuSGNOrkmcCQ"
      },
      "outputs": [],
      "source": [
        "datadir = drive + \"/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px/test/\"\n",
        "datanames = sorted(os.listdir(datadir))\n",
        "if datanames[0] == \".DS_Store\":\n",
        "    datanames.pop(0)\n",
        "print(len(datanames), \"data available...\")\n",
        "iw, ih = 210, 300\n",
        "blankframe = np.zeros((iw, ih), dtype=np.uint8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annotationfile = drive + \"/phoenix2014-release/phoenix-2014-multisigner/annotations/manual/test\"\n",
        "with open(annotationfile,\"rb\") as pf:\n",
        "    annotations = pf.readlines()\n",
        "pf.close()"
      ],
      "metadata": {
        "id": "F4kz-lLgejfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JD7k62IEhYYa"
      },
      "source": [
        "**Select test data:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRJgoxjwgQRf"
      },
      "outputs": [],
      "source": [
        "teststart = 0\n",
        "teststop = 629\n",
        "datanamespart = datanames[teststart:teststop]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhjQ40FymXGM"
      },
      "source": [
        "**Recognition proces:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDDg9i2ZmfdX"
      },
      "outputs": [],
      "source": [
        "finalresults = []\n",
        "for dataname_i, dataname in enumerate(datanamespart):\n",
        "    if dataname != \"30March_2010_Tuesday_tagesschau_default-9\":\n",
        "        continue\n",
        "    # load original data frames\n",
        "    dataloc = datadir + dataname + f\"/1/*-0.png\"\n",
        "    datafiles = sorted(glob.glob(dataloc))\n",
        "    lendatafiles = len(datafiles)\n",
        "    print(\"(\", dataname_i + 1, \"/\", len(datanamespart),\")\",\n",
        "          dataname, \"has\", lendatafiles, \"frames\")\n",
        "    imgori = [cv.imread(filename, cv.IMREAD_COLOR)\n",
        "              for filename in datafiles]\n",
        "    dataori = [cv.resize(img, (iw, ih))\n",
        "               for img in imgori]\n",
        "    peaks = segments[dataname]\n",
        "    peaks.insert(0, 0)\n",
        "    peaks.append(lendatafiles-1)\n",
        "    allmcm = load_allmcm_video(datadir, dataname, glosstype, glossp, blankframe)\n",
        "    glossresult1 = []\n",
        "\n",
        "    try:\n",
        "        glossresult1 = find_path(dataori, dataname, glossresult1, ngrams, peaks, cw, allmcm, cf, cb)\n",
        "    except BaseException as err:\n",
        "        print(err)\n",
        "\n",
        "    listgloss = [g[0] for g in glossresult1]\n",
        "\n",
        "    a = annotations[teststart + dataname_i]\n",
        "    print(a)\n",
        "    print(listgloss)\n",
        "\n",
        "    finalresults.append((dataname, listgloss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRxBLjK41PPH"
      },
      "source": [
        "**Save the results:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYi3qZjf1RRR"
      },
      "outputs": [],
      "source": [
        "with open('finalresults', 'wb') as pf:\n",
        "    pickle.dump(finalresults, pf)\n",
        "pf.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO0dG554sBGM1RLzreMwag6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}